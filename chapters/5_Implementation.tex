
\section*{Introduction}
\label{sec:impl_intro}

The implementation phase marks the transformation of theoretical design into a tangible, functioning system. For this project, every design choice—from sensor selection to model integration and mesh communication—was practically realized and tested under real-world conditions. This chapter details the full realization of our disaster-resilient communication and environmental prediction system, describing both the hardware and software implementations in a systematic and reproducible manner.

The development process was executed in modular stages to simplify debugging and improve reliability. Each subsystem—sensor acquisition, FastAPI backend, LSTM prediction engine, and Reticulum-based mesh communication—was built and validated independently before being integrated into the unified system. This modular strategy not only enhanced maintainability but also allowed parallel development by team members specializing in different domains such as hardware interfacing, software design, and network communication.

The implementation presented several challenges, including accurate sensor calibration under fluctuating conditions, achieving long-range LoRa communication despite interference, and ensuring that machine learning predictions synchronized smoothly with live sensor inputs. Each challenge was addressed through iterative testing, parameter tuning, and validation cycles, which are described throughout this chapter.

\section{Hardware Implementation}
\label{sec:hardware_impl}

\subsection{Sensor Selection and Calibration}
\label{subsec:sensor_calibration}

The first step in the system realization was to establish a reliable environmental sensing module. The sensor suite comprised five components: MQ-4 for methane detection, MQ-7 for carbon monoxide, MQ-135 for air quality (including NH\textsubscript{3} and CO\textsubscript{2}), DHT22 for temperature and humidity, and a flame sensor for fire detection. Each was chosen for its reliability, affordability, and compatibility with the ESP32 microcontroller.

Calibrating the MQ-series sensors was the most time-consuming step, as these sensors require a prolonged pre-heating period (24–48 hours) to stabilize their baseline resistance. We continuously monitored output voltages using the Arduino Serial Plotter until they converged to steady readings. For DHT22, factory calibration ensured accuracy, but we verified its readings against a laboratory-grade hygrometer and thermometer to confirm precision. The flame sensor initially produced false positives under strong sunlight, which was resolved by implementing a threshold filter in the ESP32 firmware to ignore minor infrared fluctuations.

The sensor wiring followed a carefully designed pin configuration that balanced the ESP32’s 3.3V logic level with 5V sensor operation using voltage dividers. Common grounding ensured stable reference voltage and minimized electrical noise. This attention to calibration and grounding was critical in achieving stable long-term readings during field deployment.

\subsection{ESP32 and LoRa Module Programming}
\label{subsec:esp32_programming}

The Heltec WiFi LoRa 32 V3 board formed the core of the sensing unit, integrating an ESP32 microcontroller and SX1262 LoRa transceiver on a single chip. Firmware development was performed in the Arduino IDE due to its simplicity and compatibility with ESP32 libraries.

The firmware architecture was designed around non-blocking multitasking. The main program loop runs every few seconds to read all sensors, process data, and queue messages for transmission. Analog data from MQ sensors was converted to voltage using the 12-bit ADC equation $V = (ADC\_value \times 3.3) / 4095$. These voltages were mapped to gas concentrations using empirical calibration curves determined during testing.

Data packets were structured in JSON format for compatibility with downstream Python processing:
\begin{verbatim}
{"mq4": 245, "mq7": 18, "mq135": 312, "temp": 28.5, "humidity": 65.2, "flame": 0}
\end{verbatim}

Each packet was transmitted via two channels—serial (for debugging and local logging) and LoRa (for wireless communication). The LoRa transceiver was configured for 868 MHz operation, 125 kHz bandwidth, spreading factor SF9, and transmission power of 20 dBm, offering reliable range and minimal energy consumption. These parameters were optimized through iterative testing to balance distance and throughput.

\subsection{LoRa Communication Testing and Range Validation}
\label{subsec:lora_testing}

Field testing was essential to validate LoRa’s performance under various environments. Tests were conducted in open fields, suburban areas, and dense campus zones. In clear line-of-sight conditions, stable transmission was achieved up to 8 km, while in built-up areas the range reduced to around 1.5 km due to obstruction and multipath interference. Despite these limitations, the achieved coverage comfortably exceeded our campus requirements.

Packet loss remained below 5\% in most cases. To further enhance reliability, an automatic retransmission mechanism was implemented for critical alert packets. Signal strength (RSSI) was continuously monitored to evaluate link stability and assist in node placement during deployment.

\section{Software Implementation}
\label{sec:software_impl}

\subsection{Data Acquisition via Serial Communication}
\label{subsec:serial_comm}

The bridge between hardware and software was established using the PySerial library, which allowed continuous streaming of sensor data from the ESP32’s USB port to the FastAPI server. The acquisition script was designed to automatically detect active COM ports, read data, and parse JSON packets in real time.

Robustness was a primary concern. To handle malformed or incomplete packets, we implemented error-catching mechanisms that skipped corrupted data while logging errors for debugging. A watchdog timer ensured that if no data was received within 30 seconds, the serial connection would automatically reinitialize. This design prevented crashes during long-term operation.

Additionally, an event-driven alerting mechanism was embedded within the acquisition script. When incoming readings exceeded critical thresholds—such as methane >1000 ppm or temperature >45°C—the script immediately flagged the reading as an emergency event and prioritized it for transmission to the server.

\subsection{FastAPI Backend and Database Management}
\label{subsec:fastapi_backend}

The FastAPI server served as the central coordinator of the system. It provided RESTful endpoints for receiving sensor data, performing ML inference, and delivering alerts. FastAPI’s asynchronous architecture ensured high performance even when multiple devices transmitted simultaneously.

Incoming data was validated before being stored in a local SQLite database. Invalid values (e.g., negative humidity or non-numeric entries) were rejected with informative error messages. Valid data was timestamped and stored in a structured format for time-series analysis. The database schema included tables for raw readings, predictions, alerts, and system health metrics.

FastAPI’s built-in background task feature enabled non-blocking ML predictions. Once data was received, the endpoint returned an acknowledgment immediately while the LSTM model processed data in the background. This kept the server responsive and suitable for real-time operations.

\subsection{LSTM Prediction Model Integration}
\label{subsec:lstm_model}

The intelligence layer of the system uses a Long Short-Term Memory (LSTM) model for environmental forecasting. The LSTM was trained on a Kaggle dataset containing multi-day pollution data, further augmented through noise injection and normalization.

The network consisted of two LSTM layers (128 and 64 units), each followed by dropout, and a dense output layer predicting methane, CO, and humidity levels for the next 3 hours. The model was trained with the Adam optimizer (learning rate 0.001) using Mean Squared Error loss for 50 epochs. Early stopping prevented overfitting, and the model achieved over 87\% prediction accuracy on test data.

Integration with FastAPI was achieved using TensorFlow’s SavedModel format. Predictions were automatically triggered every few minutes based on the latest readings. The system classified predicted values into three levels—Safe, Caution, and Danger—according to predefined thresholds, ensuring that potential risks were flagged before escalation.

\subsection{Reticulum Mesh Network Configuration}
\label{subsec:reticulum_config}

To enable offline alert transmission, we deployed the Reticulum mesh network on both laptops. Reticulum provides a peer-to-peer encrypted communication layer operating over LoRa interfaces. Each node generates its own cryptographic identity, eliminating the need for central authentication.

Configuration involved defining LoRa interfaces in the Reticulum config file, specifying serial ports, baud rates, and enabling message forwarding. Once configured, the two nodes (Admin and Relay) automatically discovered each other and established secure links. The bridge service, written in Python, fetched alerts from the FastAPI server and broadcast them across Reticulum in real time. Laptop B, serving as the relay node, rebroadcasted these alerts locally over Wi-Fi.

\section{System Workflow and Integration}
\label{sec:system_workflow}

The complete system workflow begins at the ESP32, which collects sensor data every few seconds. The data is transmitted to the FastAPI server for validation and storage, after which the LSTM model predicts environmental trends for the next few hours. Prediction results are checked against safety thresholds, and if danger levels are detected, an alert is generated.

The alert is then formatted into a JSON structure, encrypted, and transmitted through the Reticulum–LoRa channel to the receiving laptop. The receiver decrypts the message and serves it over a local Wi-Fi hotspot, where users can view real-time updates and system alerts through a simple web dashboard.

This seamless data flow—from sensor to user—ensures a fully operational pipeline that remains functional even in complete internet outages.

\section{Testing and Debugging}
\label{sec:testing}

Testing was conducted in stages: sensor-level, module-level, and full-system integration. Each sensor was verified using known test conditions. The MQ4 was exposed to controlled methane sources, while the flame sensor was tested with candles and sunlight. Serial and LoRa communication were stress-tested for 48 hours to verify reliability under continuous data flow.

Software testing included endpoint validation via Postman and stress testing with simulated data bursts. The LSTM predictions were compared with actual observed data to confirm accuracy. The complete system maintained an end-to-end latency under 5 seconds—well within real-time operational requirements.

\section{Deployment and Demonstration}
\label{sec:deployment}

The prototype was deployed at two strategic locations: near the Chemistry Laboratory (sensor node) and the Administrative Block (receiver node). Both nodes were powered independently with battery backups. Antennas were mounted on elevated points to improve line-of-sight communication. During demonstrations, alerts were generated in response to simulated gas leaks and rising CO levels. The system successfully transmitted predictions and emergency notifications offline within seconds, confirming the robustness of the mesh-based design.

Minor synchronization issues were encountered between sensor cycles and LSTM prediction intervals during early trials. These were resolved by aligning the sampling frequency in the firmware with the FastAPI database update interval, ensuring that the model always processed fresh data.

\section{Performance Evaluation and Optimization}
\label{sec:performance_eval}

A comprehensive evaluation was performed to assess system performance in real conditions. Parameters included sensor response time, LoRa range, network latency, and power consumption.

\begin{itemize}
    \item \textbf{Sensor Stability:} All sensors demonstrated consistent readings after calibration, with variance below 2\% across 24-hour operation.
    \item \textbf{Communication Latency:} Average message propagation delay was 1.2 seconds per packet, dominated by transmission time.
    \item \textbf{Power Efficiency:} Operating the ESP32 with deep sleep between readings extended battery life to over 10 hours on a 2000 mAh battery.
    \item \textbf{System Reliability:} The complete system ran continuously for 48 hours without critical failures.
\end{itemize}

To optimize future performance, we plan to integrate dynamic spreading factor adjustment for LoRa based on signal strength, and batch message transmission to further reduce power usage.
\begin{table}[H]
\centering
\caption{Summary of Testing Methods for Hardware and Software Components}
\label{tab:testing_summary_short}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{2.8cm}|p{3cm}|p{5cm}|p{4.8cm}|}
\hline
\textbf{Testing Type} & \textbf{ Component} & \textbf{Objective } & \textbf{Outcome Summary} \\
\hline
\textbf{Unit Testing} & Sensors (MQ4, MQ7, MQ135, DHT22, Flame) & Verify calibration, response accuracy, and reliability under different conditions (gas exposure, temperature, flame). & Stable sensor outputs achieved; flame sensor threshold optimized to reduce false alarms. \\
\hline
\textbf{Functional Testing} & ESP32 Firmware \& LoRa Module & Validate ADC conversion, JSON data formatting, and LoRa transmission logic. & Firmware stable over 24-hour run; dual serial and LoRa transmission worked without errors. \\
\hline
\textbf{Integration Testing} & ESP32–FastAPI –LoRa Pipeline & Test full data flow from sensors to backend and transmission modules. & Smooth end-to-end data flow; no data loss detected during integration runs. \\
\hline
\textbf{Software/API Testing} & FastAPI Server Endpoints & Verify input validation, concurrency handling, and database insertion accuracy. & Handled 50 req/sec without failure; accurate data validation confirmed. \\
\hline
\textbf{Model Validation} & LSTM Prediction Engine & Assess prediction accuracy for methane, CO, and humidity versus actual data. & 87\% overall accuracy; improved safety threshold reduced false negatives. \\
\hline
\textbf{System Integration} & FastAPI + LSTM + Reticulum Mesh & Ensure prediction results trigger alerts and transmit through mesh nodes. & Prediction-to-alert latency under 5 seconds; successful alert relay offline. \\
\hline
\textbf{Network Reliability} & Reticulum Mesh over LoRa & Evaluate packet delivery, encryption, and routing in offline mode. & 100\% packet success in 2-node setup; confirmed encrypted routing. \\
\hline
\textbf{Performance Testing} & End-to-End System & Continuous 48-hour test for uptime, latency, and stability. & <2\% downtime; consistent operation with automatic recovery after disconnections. \\
\hline
\textbf{Field Validation} & Full EcoSenseNet Prototype & Real-world test between Chemistry Lab and Admin Block. & Real-time alerts and predictions transmitted successfully; full campus coverage achieved. \\
\hline
\end{tabular}
\end{table}


\section{Summary}
\label{sec:impl_summary}

This implementation chapter demonstrated how each subsystem—hardware, software, communication, and intelligence—was realized, tested, and integrated into a single cohesive system. The ESP32-based sensing unit efficiently collects environmental data, the FastAPI backend manages and processes this data, the LSTM model predicts upcoming conditions, and the Reticulum mesh ensures alerts are shared offline. Through extensive testing and real-world deployment, the system proved to be reliable, accurate, and adaptable to low-infrastructure environments.

The success of this implementation validates the project’s vision: creating a low-cost, IoT-driven, AI-powered mini-satellite system capable of predicting environmental hazards and maintaining resilient communication even in complete internet outages.
